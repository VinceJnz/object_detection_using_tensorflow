{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jackliu333/object_detection_using_tensorflow/blob/main/FoodBank_Training_Customised_Image_Detection_Model_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_QgsdWbk8IuX",
    "outputId": "b2a88a65-5a2e-42c2-9fff-5434fdfcdaa5"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.9\n",
    "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORSZd8cBp4Hz"
   },
   "outputs": [],
   "source": [
    "# Codes adapted from https://github.com/nicknochnack/TFODCourse/blob/main/2.%20Training%20and%20Detection.ipynb\n",
    "# Images sampled from https://github.com/jackliu333/object_detection_using_tensorflow, with bounding boxes drawn manually for each object\n",
    "# To run using Google Colab; switch runtime type to GPU for speedup\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "NMhkoaAlJMgJ",
    "outputId": "68d89aa2-af41-4448-9888-c34b84501852"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3m-Bouz1Rde"
   },
   "source": [
    "# Configuration parameters\n",
    "\n",
    "Including paths and training hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vCm7RtDn0za5",
    "outputId": "d6c95f4d-7d9f-4327-b0c7-1e9f97c06ad8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(f\"Current OS: {os.name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6_58Cc11pRG"
   },
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet' \n",
    "\n",
    "# SSD has good tradeoff between speed and accuracy; can switch to other pretrained model\n",
    "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "\n",
    "# TF official script to encode training data to tf record format\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "\n",
    "# Mapping dictionary between label and integer id\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'\n",
    "\n",
    "# Define a list of folder paths to be created (if needed) and used later\n",
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
    "    # bounding box annotation\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'PROTOC_PATH':os.path.join('Tensorflow','protoc')\n",
    "}\n",
    "\n",
    "training_files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfIa-vW48SJR"
   },
   "outputs": [],
   "source": [
    "# create folder paths if not exist\n",
    "for p in paths.values():\n",
    "    if not os.path.exists(p):\n",
    "        !mkdir -p {p}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQLhQWeWGLwc"
   },
   "source": [
    "# Set up TF model training logistics\n",
    "\n",
    "### Download TF model training utility scripts from TF model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fupwxPvIFsBe",
    "outputId": "89808890-64b5-4dd2-fcc3-a2b197f52f59"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(paths['APIMODEL_PATH'], 'research', 'objection_detection')):\n",
    "    !git clone https://github.com/tensorflow/models {paths['APIMODEL_PATH']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXsp4NM_HQn1"
   },
   "source": [
    "### Install TF object detection library\n",
    "\n",
    "May hit some error during installation, can be ignored, as long as it returns \"OK\" in the last line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcpEbkoyGjdN",
    "outputId": "914d5f77-550e-4d47-c536-019c7fbec905"
   },
   "outputs": [],
   "source": [
    "if os.name=='posix':  \n",
    "    !apt-get install protobuf-compiler\n",
    "    !cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b_j_S-uHPzC",
    "outputId": "ab2be3de-55cc-446d-d752-646f661616e8"
   },
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
    "!python {VERIFICATION_SCRIPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGhM6ZXbJP3b"
   },
   "outputs": [],
   "source": [
    "# test if TF object detection library could be loaded\n",
    "import object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaROJKBSKJzy"
   },
   "outputs": [],
   "source": [
    "# reinstall TF and restart runtime\n",
    "# !pip install tensorflow==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j-GK26pKcm2"
   },
   "source": [
    "### Download and decompress TF pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9wR8-jt-KNFn",
    "outputId": "733e5c93-479b-4ac7-ada9-a310e1ee4f12"
   },
   "outputs": [],
   "source": [
    "if os.name =='posix':\n",
    "    !wget {PRETRAINED_MODEL_URL}\n",
    "    !mv {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
    "    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eJQTu74vDpA"
   },
   "source": [
    "### Download training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HrOfuCwavHBJ",
    "outputId": "8dd2e944-5c5c-40cb-d193-6621c9437e9f"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists('object_detection_using_tensorflow'):\n",
    "    shutil.rmtree('object_detection_using_tensorflow')\n",
    "\n",
    "!git clone https://github.com/jackliu333/object_detection_using_tensorflow.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdyE_2lbPOKH"
   },
   "source": [
    "### Create label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuPgTDHNK0P7"
   },
   "outputs": [],
   "source": [
    "# labels = [{'name':'Apple', 'id':1},\n",
    "#           {'name':'Avocado', 'id':2},\n",
    "#           {'name':'Banana', 'id':3},\n",
    "#           {'name':'Cabbage', 'id':4},\n",
    "#           {'name':'Carrot', 'id':5},\n",
    "#           {'name':'Potato', 'id':6}]\n",
    "# labels = [{'name':'Canned_Sardines_and_Mackerel', 'id':1}, \n",
    "#           {'name':'Canned_Baked_Beans', 'id':2}, \n",
    "#           {'name':'Biscuits', 'id':3},\n",
    "#           {'name':'Halal', 'id':4},\n",
    "#           {'name':'Kitkat', 'id':5},\n",
    "#           {'name':'Wafers', 'id':6}]\n",
    "\n",
    "labels = [{'name':'cookies_1g-99g', 'id':1}, \n",
    "          {'name':'cookies_1kg-1.99kg', 'id':2}, \n",
    "          {'name':'cookies_100g-199g', 'id':3},\n",
    "          {'name':'cookies_200g-299g', 'id':4},\n",
    "          {'name':'crackers_100g-199g', 'id':5},\n",
    "          {'name':'crackers_200g-299g', 'id':6},\n",
    "          {'name':'crackers_300g-399g', 'id':7},\n",
    "          {'name':'crackers_400g-499g', 'id':8},\n",
    "          {'name':'sardines_100g-199g', 'id':9},\n",
    "          {'name':'sardines_400g-499g', 'id':10},\n",
    "          {'name':'Baked beans_400-499g', 'id':11},\n",
    "          {'name':'cookies_300g-399g', 'id':12},\n",
    "          {'name':'cookies_400g-499g', 'id':13},\n",
    "          {'name':'cookies_500g-599g', 'id':14},\n",
    "          {'name':'cookies_600g-699g', 'id':15},\n",
    "          {'name':'crackers_1g-99g', 'id':16},\n",
    "          {'name':'crackers_800g-899g', 'id':17},\n",
    "          {'name':'halal', 'id':18}]\n",
    "\n",
    "# labels = [{'name':'cookies_1g-99g', 'id':1}, \n",
    "#           {'name':'cookies_1kg-1.99kg', 'id':2},\n",
    "#           {'name':'cookies_100g-199g', 'id':3},\n",
    "#           {'name':'cookies_200g-299g', 'id':4},\n",
    "#           {'name':'crackers_100g-199g', 'id':5},\n",
    "#           {'name':'crackers_200g-299g', 'id':6},\n",
    "#           {'name':'crackers_300g-399g', 'id':7},\n",
    "#           {'name':'crackers_400g-499g', 'id':8}]\n",
    "\n",
    "# labels = [{'name':'Baked beans_400-499g', 'id':1}]\n",
    "\n",
    "with open(training_files['LABELMAP'], 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEv7XDo7nlpK"
   },
   "source": [
    "### Split into train test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBJcyZa-rM42"
   },
   "outputs": [],
   "source": [
    "tmp_folders = ['train', 'test']\n",
    "\n",
    "for i in tmp_folders:\n",
    "    if os.path.exists(os.path.join(paths['IMAGE_PATH'], i)):\n",
    "        shutil.rmtree(os.path.join(paths['IMAGE_PATH'], i))\n",
    "        !mkdir -p {os.path.join(paths['IMAGE_PATH'], i)}\n",
    "    else:\n",
    "        !mkdir -p {os.path.join(paths['IMAGE_PATH'], i)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5boDfQnanvSJ"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    # print(labels[i]['name'])\n",
    "    from_path = os.path.join('object_detection_using_tensorflow','images5',labels[i]['name'])\n",
    "    # print(from_path)\n",
    "\n",
    "    # get unique file names\n",
    "    tmp_files = os.listdir(from_path)\n",
    "    tmp_names = []\n",
    "    tmp_file_types = []\n",
    "    for tmp_file in tmp_files:\n",
    "        tmp_name = os.path.splitext(tmp_file)[0]\n",
    "        tmp_file_type = os.path.splitext(tmp_file)[1]\n",
    "        tmp_names.append(tmp_name)\n",
    "        tmp_file_types.append(tmp_file_type)\n",
    "    tmp_names = list(set(tmp_names))\n",
    "    tmp_names = [i for i in tmp_names if i != '.DS_Store']\n",
    "    tmp_file_types = list(set(tmp_file_types))\n",
    "    tmp_file_types = [i for i in tmp_file_types if len(i) != 0]\n",
    "    # random shuffle the files\n",
    "    random.shuffle(tmp_names)\n",
    "    \n",
    "    # training and test files\n",
    "    tmp_names_train = tmp_names[0:int(len(tmp_names)*0.9)]\n",
    "    tmp_names_test = [i for i in tmp_names if i not in tmp_names_train]\n",
    "\n",
    "    # move into respective target folders\n",
    "    for tmp_name in tmp_names_train:\n",
    "        for tmp_file_type in tmp_file_types:\n",
    "            tmp_name_full = tmp_name + tmp_file_type\n",
    "            if(os.path.exists(os.path.join(from_path, tmp_name_full))):\n",
    "                shutil.copy(os.path.join(from_path, tmp_name_full), \\\n",
    "                            os.path.join(paths['IMAGE_PATH'], \"train\"))\n",
    "\n",
    "    for tmp_name in tmp_names_test:\n",
    "        for tmp_file_type in tmp_file_types:\n",
    "            tmp_name_full = tmp_name + tmp_file_type\n",
    "            if(os.path.exists(os.path.join(from_path, tmp_name_full))):    \n",
    "                shutil.copy(os.path.join(from_path, tmp_name_full), \\\n",
    "                            os.path.join(paths['IMAGE_PATH'], \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLTd2_pJdxFF"
   },
   "outputs": [],
   "source": [
    "# from_path\n",
    "# os.listdir(from_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI2Xr8VAh3eX"
   },
   "source": [
    "### Create TF Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7HHaNqVqh5tg",
    "outputId": "66513da8-9d72-4d37-b35e-23433ee15692"
   },
   "outputs": [],
   "source": [
    "# download conversion script\n",
    "if not os.path.exists(training_files['TF_RECORD_SCRIPT']):\n",
    "    !git clone https://github.com/nicknochnack/GenerateTFRecord {paths['SCRIPTS_PATH']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AppAbu0aiGFA",
    "outputId": "31ee732a-3b40-4557-95ca-11d787920f3e"
   },
   "outputs": [],
   "source": [
    "# convert to TF record format\n",
    "!python {training_files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'train')} -l {training_files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'train.record')} \n",
    "!python {training_files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'test')} -l {training_files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'test.record')} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3XsE4cQgMp5"
   },
   "source": [
    "### Copy model configuration file to training folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmNNSD6RQT3m"
   },
   "outputs": [],
   "source": [
    "!cp {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {paths['CHECKPOINT_PATH']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JupJmSHyUSz"
   },
   "source": [
    "### Update configuration file for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBWbfpEexiwZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "# Read current configuration file\n",
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(training_files['PIPELINE_CONFIG'], \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config)  \n",
    "\n",
    "# Update based on new labels\n",
    "pipeline_config.model.ssd.num_classes = len(labels)\n",
    "pipeline_config.train_config.batch_size = 4\n",
    "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= training_files['LABELMAP']\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = training_files['LABELMAP']\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]\n",
    "\n",
    "# # Add more data augmentation\n",
    "# upd_dict = {'data_augmentation_options':{'random_rgb_to_gray' : {}}}\n",
    "# # pipeline_config.train_config = upd_dict\n",
    "# pipeline_config.train_config.data_augmentation_options.random_rgb_to_gray = []\n",
    "# # Load the pipeline config as a dictionary\n",
    "# pipeline_config_dict = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "# # update dict entry\n",
    "\n",
    "# pipeline_config_dict[\"train_config\"].update(upd_dict)\n",
    "# # Convert the pipeline dict back to a protobuf object\n",
    "# pipeline_config = config_util.create_pipeline_proto_from_configs(pipeline_config_dict)\n",
    "\n",
    "# Write to configuration file\n",
    "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(training_files['PIPELINE_CONFIG'], \"wb\") as f:                                                                                                                                                                                                                     \n",
    "    f.write(config_text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b4MBsUO0koz"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4q9TuKHHlxtj",
    "outputId": "f7098770-025f-4783-bd66-2aaf5dd1fa2f"
   },
   "outputs": [],
   "source": [
    "# fix opencv-python version issue \n",
    "!pip uninstall opencv-python-headless \n",
    "!pip install opencv-python-headless==4.1.2.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgMB7YINkozI"
   },
   "outputs": [],
   "source": [
    "# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsiEJBXkzwtF"
   },
   "outputs": [],
   "source": [
    "# # import fileinput\n",
    "# file_path = '/usr/local/lib/python3.7/dist-packages/official/modeling/optimization/optimizer_factory.py'\n",
    "# # fields = {\"adamw_experimental\": \"tf.keras.optimizers.Adam\"}\n",
    "\n",
    "# with open(file_path, 'r') as file:\n",
    "  \n",
    "#     data = file.read()\n",
    "#     data = data.replace('tf.keras.optimizers.experimental.AdamW', 'tf.keras.optimizers.AdamW')\n",
    " \n",
    "# with open(file_path, 'w') as file:\n",
    "  \n",
    "#     # Writing the replaced data in our\n",
    "#     # text file\n",
    "#     file.write(data)\n",
    "\n",
    "# # print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tt9OuLm10o2G",
    "outputId": "435afe7b-1fb0-4c20-d798-53745afc8fae"
   },
   "outputs": [],
   "source": [
    "training_steps = 30000\n",
    "check_point = 'ckpt-31'\n",
    "TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')\n",
    "command = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps={}\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],training_files['PIPELINE_CONFIG'],training_steps)\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "MaOMSxtQX4x4",
    "outputId": "50cd85db-3a0b-4fde-9247-ebe2fe0b6c2c"
   },
   "outputs": [],
   "source": [
    "training_files['PIPELINE_CONFIG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpbSrRl4Bjou"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "tOo6P6FrEwf2",
    "outputId": "9cdc3041-54f0-4e96-8b74-0955af7667b7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w81ZW7isqWRZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-YemFHFBjf3"
   },
   "outputs": [],
   "source": [
    "# with open('/usr/local/lib/python3.7/dist-packages/official/modeling/optimization/optimizer_factory.py', 'r') as f:\n",
    "    # print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrXjxgnp4I5t"
   },
   "source": [
    "# (Optional) Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0huXcblU0uYm"
   },
   "outputs": [],
   "source": [
    "# # stop evaluation if needed\n",
    "# command = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'])\n",
    "# !{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXU31dxv5hwP"
   },
   "source": [
    "# Predicting object category in image\n",
    "\n",
    "### Load trained model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7a5XB5pQ4RzX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(training_files['PIPELINE_CONFIG'])\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], check_point)).expect_partial()\n",
    "\n",
    "# @tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "0t08NET2OgML",
    "outputId": "208dbeef-9cfe-42f6-d7be-d5ae85b68249"
   },
   "outputs": [],
   "source": [
    "paths['CHECKPOINT_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xMF_w1854yf"
   },
   "source": [
    "### Detect objects in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "hCDa4sGs534v",
    "outputId": "628af89b-dd99-4d34-9a82-9bc45d47a05c"
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "# Randomly select an image to be detected\n",
    "# tmp_img = random.choice([file for file in os.listdir(os.path.join(paths['IMAGE_PATH'], \n",
    "#                                               'test')) if file.endswith(\".JPG\")])\n",
    "# IMAGE_PATH = os.path.join(paths['IMAGE_PATH'], 'test', tmp_img)\n",
    "tmp_img = random.choice([file for file in os.listdir(os.path.join(paths['IMAGE_PATH'], \n",
    "                                              'train')) if file.endswith(\".png\")])\n",
    "IMAGE_PATH = os.path.join(paths['IMAGE_PATH'], 'train', tmp_img)\n",
    "IMAGE_PATH = './crackers_100-199g_H_1.jpg'\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(training_files['LABELMAP'])\n",
    "\n",
    "img = cv2.imread(IMAGE_PATH)\n",
    "image_np = np.array(img)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "detections = detect_fn(input_tensor)\n",
    "\n",
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy()\n",
    "              for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections\n",
    "\n",
    "# detection_classes should be ints.\n",
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "label_id_offset = 1\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np_with_detections,\n",
    "            detections['detection_boxes'],\n",
    "            detections['detection_classes']+label_id_offset,\n",
    "            detections['detection_scores'],\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=2,\n",
    "            min_score_thresh=.7,\n",
    "            agnostic_mode=False,\n",
    "            line_thickness=8)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYTGhwI-greW",
    "outputId": "dab0fc7f-9afc-490f-effe-4c42ac6c4877"
   },
   "outputs": [],
   "source": [
    "print(\"detected class:\")\n",
    "print(category_index[detections['detection_classes'][0]+label_id_offset]['name'])\n",
    "print(\"detected probability\")\n",
    "print(detections['detection_scores'][0])\n",
    "print(\"true class\")\n",
    "print(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1Pj1Mhj67Qgm",
    "outputId": "eefc87e8-f9ad-4bf1-b7c1-da315c05a607"
   },
   "outputs": [],
   "source": [
    "!zip -r Tensorflow.zip Tensorflow/\n",
    "from google.colab import files\n",
    "files.download(\"/content/Tensorflow.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "kFAz5lpY5oIL",
    "outputId": "1ab4dc62-e409-4142-c09c-09acff57d3d1"
   },
   "outputs": [],
   "source": [
    "# files.download(\"/content/Tensorflow.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhJV7eBLWD75"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNYLucD3jUB3wCEUAMVivQd",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
