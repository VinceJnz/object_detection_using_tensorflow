{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jackliu333/object_detection_using_tensorflow/blob/main/Training_Customised_Image_Detection_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "https://planspace.org/20170323-tfrecords_for_humans/\n",
    "\n",
    "https://github.com/tensorflow/models/tree/master/research/object_detection\n",
    "    \n",
    "https://medium.com/practical-deep-learning/a-complete-transfer-learning-toolchain-for-semantic-segmentation-3892d722b604\n",
    "\n",
    "https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ORSZd8cBp4Hz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current OS: nt\n"
     ]
    }
   ],
   "source": [
    "# Codes adapted from https://github.com/nicknochnack/TFODCourse/blob/main/2.%20Training%20and%20Detection.ipynb\n",
    "# Images sampled from https://github.com/marcusklasson/GroceryStoreDataset, with bounding boxes drawn manually for each object\n",
    "# To run using Google Colab; switch runtime type to GPU for speedup\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "import os\n",
    "print(f\"Current OS: {os.name}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3m-Bouz1Rde"
   },
   "source": [
    "# Configuration parameters\n",
    "\n",
    "Including paths and training hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow 2 Detection Model Zoo\n",
    "\n",
    "Getting other pretrained models\n",
    "\n",
    "https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/plot_object_detection_saved_model.html\n",
    "\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V6_58Cc11pRG"
   },
   "outputs": [],
   "source": [
    "#CUSTOM_MODEL_NAME = 'my_ssd_mobnet'\n",
    "# SSD has good tradeoff between speed and accuracy; can switch to other pretrained model\n",
    "#PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "#PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "\n",
    "\n",
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet_640'\n",
    "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz'\n",
    "\n",
    "\n",
    "#CUSTOM_MODEL_NAME = 'my_efficientdet_d4'\n",
    "#PRETRAINED_MODEL_NAME = 'efficientdet_d4_coco17_tpu-32'\n",
    "#PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d4_coco17_tpu-32.tar.gz'\n",
    "\n",
    "\n",
    "#CUSTOM_MODEL_NAME = 'my_efficientdet_d0'\n",
    "#PRETRAINED_MODEL_NAME = 'efficientdet_d0_coco17_tpu-32'\n",
    "#PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz'\n",
    "\n",
    "\n",
    "# TF official script to encode training data to tf record format\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "\n",
    "# Mapping dictionary between label and integer id\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'\n",
    "\n",
    "TENSORFLOW = '..'\n",
    "\n",
    "# Define a list of folder paths to be created (if needed) and used later\n",
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
    "    #'IMAGE_SOURCE_PATH':os.path.join('images'),\n",
    "    'IMAGE_SOURCE_PATH': os.path.join('E:', os.sep,'Users','Vince','Datasets','NZRC','Gordon','CycloneGitaRawImagery','ML4DR_20210910_01','tiles'),\n",
    "    \n",
    "    #'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
    "    'ANNOTATION_PATH': os.path.join('E:', os.sep,'Users','Vince','Datasets','NZRC','Gordon','CycloneGitaRawImagery','ML4DR_20210910_01','tiles'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'PROTOC_PATH': os.path.join('Tensorflow','protoc')\n",
    "}\n",
    "\n",
    "files = {\n",
    "    'PIPELINE_CONFIG': os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfIa-vW48SJR"
   },
   "outputs": [],
   "source": [
    "# create folder paths if not exist\n",
    "for p in paths.values():\n",
    "    if not os.path.exists(p):\n",
    "        !mkdir -p {p}\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an image from a TF Record with its bounding boxes and labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plt_image_with_labels(image, annotations, categories):\n",
    "    ## Display the image, with bboxes and labels\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "    height = annotations[\"image/height\"]\n",
    "    width = annotations[\"image/width\"]\n",
    "    # Create Rectangle patches, i.e. bbox add rectangles to the image\n",
    "    for j in range(len(annotations['image/object/class/label'])):\n",
    "        x1 = annotations[\"image/object/bbox/xmin\"][j] * width\n",
    "        y1 = annotations[\"image/object/bbox/ymin\"][j] * height\n",
    "        x2 = annotations[\"image/object/bbox/xmax\"][j] * width - x1\n",
    "        y2 = annotations[\"image/object/bbox/ymax\"][j] * height - y1\n",
    "        \n",
    "        category_num = annotations[\"image/object/class/label\"][j]-1\n",
    "        label = categories[category_num][\"name\"]\n",
    "        rect = patches.Rectangle((x1, y1), x2, y2, linewidth=1, edgecolor='red', facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1,y1, label, horizontalalignment='left', fontsize=8, color='red')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an image from a TF Record with its bounding boxes and labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plt_image_with_labels2(image, annotations, categories):\n",
    "    ## Display the image, with bboxes and labels\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "    height = annotations['height']\n",
    "    width = annotations['width']\n",
    "    # Create Rectangle patches, i.e. bbox add rectangles to the image\n",
    "    for row in annotations['annotations']:\n",
    "        x1, y1, x2, y2 = row['bbox']\n",
    "        \n",
    "        category_num = row['category_id']-1\n",
    "        label = categories[category_num][\"name\"]\n",
    "        rect = patches.Rectangle((x1, y1), x2, y2, linewidth=1, edgecolor='red', facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1,y1, label, horizontalalignment='left', fontsize=8, color='red')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lable map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_map(lables, path=files['LABELMAP']):\n",
    "    with open(files['LABELMAP'], 'w') as f:\n",
    "        for label in labels:\n",
    "            f.write('item { \\n')\n",
    "            f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "            f.write('\\tid:{}\\n'.format(label['id']))\n",
    "            f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQLhQWeWGLwc"
   },
   "source": [
    "# Set up TF model training logistics\n",
    "\n",
    "### Download TF model training utility scripts from TF model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fupwxPvIFsBe",
    "outputId": "950331c2-7e3a-4626-e399-73aaf5e97a72"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(paths['APIMODEL_PATH'], 'research', 'objection_detection')):\n",
    "    !git clone https://github.com/tensorflow/`models {paths['APIMODEL_PATH']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXsp4NM_HQn1"
   },
   "source": [
    "### Install TF object detection library\n",
    "\n",
    "May hit some error during installation, can be ignored, as long as it returns \"OK\" in the last line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linux version\n",
    "if os.name=='posix':  \n",
    "    !apt-get install protobuf-compiler\n",
    "    !cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. \n",
    "    !cd Tensorflow/models/research && cp object_detection/packages/tf2/setup.py . && python -m pip install . \n",
    "\n",
    "\n",
    "#windows version\n",
    "if os.name =='nt':\n",
    "    !cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=.\n",
    "    !cd Tensorflow/models/research && copy object_detection\\packages\\tf2\\setup.py . && python -m pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b_j_S-uHPzC",
    "outputId": "7179bd3e-cc0a-469d-a5a8-1d67e87113df"
   },
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
    "!python {VERIFICATION_SCRIPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DGhM6ZXbJP3b"
   },
   "outputs": [],
   "source": [
    "# test if TF object detection library could be loaded\n",
    "import object_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j-GK26pKcm2"
   },
   "source": [
    "### Download and decompress TF pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linux version\n",
    "if os.name =='posix':\n",
    "    !wget {PRETRAINED_MODEL_URL}\n",
    "    !mv {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
    "    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}\n",
    "\n",
    "#Windows version\n",
    "if os.name =='nt':\n",
    "    !wget {PRETRAINED_MODEL_URL}\n",
    "    !move {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
    "    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdyE_2lbPOKH"
   },
   "source": [
    "### Create label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xuPgTDHNK0P7"
   },
   "outputs": [],
   "source": [
    "#labels = [{'name':'Apple', 'id':1},\n",
    "#          {'name':'Avocado', 'id':2},\n",
    "#          {'name':'Banana', 'id':3},\n",
    "#          {'name':'Cabbage', 'id':4},\n",
    "#          {'name':'Carrot', 'id':5},\n",
    "#          {'name':'Potato', 'id':6}]\n",
    "\n",
    "labels = [{'name':'None', 'id':1},\n",
    "          {'name':'<25%', 'id':2},\n",
    "          {'name':'>25%', 'id':3},\n",
    "          {'name':'Abandoned', 'id':4},\n",
    "          {'name':'Other', 'id':5}]\n",
    "\n",
    "create_label_map(labels, files['LABELMAP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eJQTu74vDpA"
   },
   "source": [
    "### Download training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HrOfuCwavHBJ",
    "outputId": "45a233b4-cd60-4608-a404-7718febf7c29"
   },
   "outputs": [],
   "source": [
    "#Not needed\n",
    "#import shutil\n",
    "\n",
    "#if os.path.exists('object_detection_using_tensorflow'):\n",
    "#    shutil.rmtree('object_detection_using_tensorflow')\n",
    "\n",
    "#!git clone https://github.com/jackliu333/object_detection_using_tensorflow.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEv7XDo7nlpK"
   },
   "source": [
    "### Split into train test folders\n",
    "\n",
    "Don't use this for COCO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create image destination folders\n",
    "import shutil\n",
    "\n",
    "tmp_folders = ['train', 'test']\n",
    "\n",
    "#windows version\n",
    "if os.name =='nt':\n",
    "    for i in tmp_folders:\n",
    "        if os.path.exists(os.path.join(paths['IMAGE_PATH'], i)):\n",
    "            shutil.rmtree(os.path.join(paths['IMAGE_PATH'], i))\n",
    "            !mkdir {os.path.join(paths['IMAGE_PATH'], i)}\n",
    "        else:\n",
    "            !mkdir {os.path.join(paths['IMAGE_PATH'], i)}\n",
    "#Linux\n",
    "if os.name =='posix':\n",
    "    for i in tmp_folders:\n",
    "        if os.path.exists(os.path.join(paths['IMAGE_PATH'], i)):\n",
    "            shutil.rmtree(os.path.join(paths['IMAGE_PATH'], i))\n",
    "            !mkdir -p {os.path.join(paths['IMAGE_PATH'], i)}\n",
    "        else:\n",
    "            !mkdir -p {os.path.join(paths['IMAGE_PATH'], i)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5boDfQnanvSJ"
   },
   "outputs": [],
   "source": [
    "#Copy images from source folders to destination folders\n",
    "# Create a list of image file paths from \"image_source_path + label name\"\n",
    "# This assume the images are stored in folders with the category (class) name\n",
    "import shutil\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    from_path = os.path.join(paths['IMAGE_SOURCE_PATH'],labels[i]['name'])\n",
    "\n",
    "    # get unique file names\n",
    "    tmp_files = os.listdir(from_path)\n",
    "    tmp_names = []\n",
    "    tmp_file_types = []\n",
    "    for tmp_file in tmp_files:\n",
    "        tmp_name = os.path.splitext(tmp_file)[0]\n",
    "        tmp_file_type = os.path.splitext(tmp_file)[1]\n",
    "        tmp_names.append(tmp_name)\n",
    "        tmp_file_types.append(tmp_file_type)\n",
    "    tmp_names = list(set(tmp_names))\n",
    "    tmp_names = [i for i in tmp_names if i != '.DS_Store']\n",
    "    tmp_file_types = list(set(tmp_file_types))\n",
    "    tmp_file_types = [i for i in tmp_file_types if len(i) != 0]\n",
    "    # random shuffle the files\n",
    "    random.shuffle(tmp_names)\n",
    "    \n",
    "    # training and test files\n",
    "    tmp_names_train = tmp_names[0:int(len(tmp_names)*0.9)]\n",
    "    tmp_names_test = [i for i in tmp_names if i not in tmp_names_train]\n",
    "\n",
    "    # move into respective target folders\n",
    "    for tmp_name in tmp_names_train:\n",
    "        for tmp_file_type in tmp_file_types:\n",
    "            tmp_name_full = tmp_name + tmp_file_type\n",
    "            shutil.copy(os.path.join(from_path, tmp_name_full), \\\n",
    "                        os.path.join(paths['IMAGE_PATH'], \"train\"))\n",
    "\n",
    "    for tmp_name in tmp_names_test:\n",
    "        for tmp_file_type in tmp_file_types:\n",
    "            tmp_name_full = tmp_name + tmp_file_type\n",
    "            shutil.copy(os.path.join(from_path, tmp_name_full), \\\n",
    "                        os.path.join(paths['IMAGE_PATH'], \"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep COCO file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.join('E:', os.sep ,'Users', 'Vince', 'Datasets', 'NZRC', 'Gordon', 'CycloneGitaRawImagery', 'ML4DR_20210910_01')\n",
    "tfrecords_dir = os.path.join(root_dir, 'tfrecords/train')\n",
    "images_dir = os.path.join(root_dir, 'tiles')\n",
    "annotations_dir = os.path.join(root_dir, 'tiles')\n",
    "#annotation_file = os.path.join(annotations_dir, \"ML4DR_20210910_01_coco.json\")\n",
    "annotation_file = os.path.join(annotations_dir, 'ML4DR_20210910_01_coco_tiled.json')\n",
    "paths['IMAGE_PATH'] = images_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths['IMAGE_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load COCO file information\n",
    "\n",
    "import json\n",
    "\n",
    "#Load image refs from COCO file\n",
    "with open(annotation_file, \"r\") as f:\n",
    "    coco_images = json.load(f)[\"images\"]\n",
    "\n",
    "#Load annotations from COCO file\n",
    "with open(annotation_file, \"r\") as f:\n",
    "    coco_annotations = json.load(f)[\"annotations\"]\n",
    "    \n",
    "#Load categories from COCO file\n",
    "with open(annotation_file, \"r\") as f:\n",
    "    coco_categories = json.load(f)[\"categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange COCO file info into a form that can be used to create the TF Record file\n",
    "# this requires the annotation info for each image be associated to the image info\n",
    "# and it requires a category dictionary to convert from category_id to category_text\n",
    "\n",
    "#Work through coco-images\n",
    "# Extract the image attributes\n",
    "# For each image get a list of annotations\n",
    "# Add the annotations list to the image\n",
    "\n",
    "imageInfoList = []\n",
    "for image_info in coco_images:\n",
    "    annotationsList = []\n",
    "    for annotation in coco_annotations:\n",
    "        if image_info['id'] == annotation['image_id']:\n",
    "            annotationsList.append(annotation)\n",
    "    if len(annotationsList)>0:\n",
    "        image = image_info\n",
    "        image['annotations'] = annotationsList\n",
    "        imageInfoList.append(image)\n",
    "        \n",
    "categories = {}\n",
    "labels = []\n",
    "for cat in coco_categories:\n",
    "    labels.append({'name':cat['name'], 'id':cat['id']})\n",
    "    categories[cat['id']] = cat['name']\n",
    "    \n",
    "create_label_map(labels, files['LABELMAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is used when creating and reading the TF Record files\n",
    "\n",
    "from object_detection.utils import dataset_util #, label_map_util\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "#This is used write the TF Records\n",
    "#Store images with its associated annotations in a TF Record file\n",
    "def create_tf_example(imageInfo, image_path):\n",
    "    with tf.io.gfile.GFile(os.path.join(image_path, '{}'.format(imageInfo['file_name'])), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "\n",
    "    image_id = imageInfo['id']\n",
    "    image_width  = int(imageInfo['width'])\n",
    "    image_height = int(imageInfo['height'])\n",
    "\n",
    "    filename = imageInfo['file_name']\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    category_names = []\n",
    "    category_ids = []\n",
    "    num_annotations_skipped = 0\n",
    "    for row in imageInfo['annotations']:\n",
    "        (x, y, width, height) = tuple(row['bbox'])\n",
    "        if x + width > image_width or y + height > image_height:\n",
    "            num_annotations_skipped += 1\n",
    "            continue\n",
    "        xmins.append(x / image_width)\n",
    "        xmaxs.append((x + width) / image_width)\n",
    "        ymins.append(y / image_height)\n",
    "        ymaxs.append((y + height) / image_height)\n",
    "        category_id = int(row['category_id'])\n",
    "        category_names.append(categories[category_id].encode('utf8'))\n",
    "        category_ids.append(category_id)\n",
    "\n",
    "    feature = {\n",
    "        'image/height': dataset_util.int64_feature(image_height),\n",
    "        'image/width': dataset_util.int64_feature(image_width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename.encode('utf8')),\n",
    "        'image/source_id': dataset_util.bytes_feature(str(image_id).encode('utf8')),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(category_names), #?????\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(category_ids),\n",
    "    }\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    "#This is used read the TF Records\n",
    "#<https://www.tensorflow.org/api_docs/python/tf/io/FixedLenSequenceFeature>\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True, default_value=None),\n",
    "        'image/object/bbox/xmax': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True, default_value=None),\n",
    "        'image/object/bbox/ymin': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True, default_value=None),\n",
    "        'image/object/bbox/ymax': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True, default_value=None),\n",
    "        'image/object/class/text': tf.io.FixedLenSequenceFeature([], tf.string, allow_missing=True, default_value=None),\n",
    "        'image/object/class/label': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True, default_value=None),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Record from COCO files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF Record files\n",
    "output_path = os.path.join(paths['ANNOTATION_PATH'],'test.record')\n",
    "writer = tf.io.TFRecordWriter(output_path)\n",
    "\n",
    "for imageInfo in imageInfoList:\n",
    "    tf_example = create_tf_example(imageInfo, paths['IMAGE_SOURCE_PATH'])\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "\n",
    "output_path = os.path.join(paths['ANNOTATION_PATH'],'train.record')\n",
    "writer = tf.io.TFRecordWriter(output_path)\n",
    "\n",
    "for imageInfo in imageInfoList:\n",
    "    tf_example = create_tf_example(imageInfo, paths['IMAGE_SOURCE_PATH'])\n",
    "    writer.write(tf_example.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(paths['ANNOTATION_PATH'],'train.record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read TFRecord files and Plot an image with the label boxes\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(os.path.join(paths['ANNOTATION_PATH'], 'train.record'))\n",
    "parsed_dataset = raw_dataset.map(parse_tfrecord_fn)\n",
    "\n",
    "sample = parsed_dataset.take(2)\n",
    "#sample = random.choice([file for file in os.listdir(os.path.join(paths['IMAGE_PATH'], '')) if file.endswith(\".JPG\")])\n",
    "\n",
    "for item in sample.as_numpy_iterator():\n",
    "    encoded_jpg = item['image/encoded']\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    image_np = np.array(image)\n",
    "    plt_image_with_labels(image_np, item, coco_categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3XsE4cQgMp5"
   },
   "source": [
    "### Copy model configuration file to training folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')})\n",
    "\n",
    "print(os.path.join(paths['CHECKPOINT_PATH'], 'pipeline.config'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmNNSD6RQT3m"
   },
   "outputs": [],
   "source": [
    "if os.name =='posix':\n",
    "    !cp {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {paths['CHECKPOINT_PATH']}\n",
    "    \n",
    "if os.name =='nt':\n",
    "    !mkdir {os.path.join(paths['CHECKPOINT_PATH'])}\n",
    "    !copy {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'], 'pipeline.config')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JupJmSHyUSz"
   },
   "source": [
    "### Update configuration file for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PRETRAINED_MODEL_NAME =', PRETRAINED_MODEL_NAME)\n",
    "print('pipeline_config.train_config.fine_tune_checkpoint =')\n",
    "print('     ' , os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0'))\n",
    "print('MODEL_PATH =', paths['MODEL_PATH'])\n",
    "print()\n",
    "print('python ', TRAINING_SCRIPT)\n",
    "print('   --model_dir={} ', paths['CHECKPOINT_PATH'])\n",
    "print('   --pipeline_config_path={} ', files['PIPELINE_CONFIG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to load an partially fine tuned model ????????\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "# Read current configuration file\n",
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:\n",
    "    proto_str = f.read()\n",
    "    text_format.Merge(proto_str, pipeline_config)\n",
    "\n",
    "# Update based on new labels\n",
    "pipeline_config.model.ssd.num_classes = len(labels)\n",
    "#pipeline_config.train_config.batch_size = 4\n",
    "#pipeline_config.train_config.batch_size = 3\n",
    "#pipeline_config.train_config.batch_size = 2\n",
    "\n",
    "#if the batch size is small the learning rate needs to be reduced for the model to converge\n",
    "#https://github.com/tensorflow/models/issues/9151\n",
    "#https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e\n",
    "#pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.learning_rate_base = 0.05 #0.08\n",
    "#pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.warmup_learning_rate = 0.001 #0.001\n",
    "#pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['CHECKPOINT_PATH'], 'checkpoint1', 'ckpt-3')\n",
    "#pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['CHECKPOINT_PATH'], 'checkpoint2', 'ckpt-3')\n",
    "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['CHECKPOINT_PATH'], 'checkpoint2')\n",
    "#pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['CHECKPOINT_PATH'], 'checkpoint3', 'ckpt-3')\n",
    "#pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['CHECKPOINT_PATH'], 'checkpoint4', 'ckpt-3')\n",
    "#pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['CHECKPOINT_PATH'], 'checkpoint5', 'ckpt-3')\n",
    "#pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['CHECKPOINT_PATH'], 'checkpoint6', 'ckpt-3')\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= files['LABELMAP']\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]\n",
    "pipeline_config.eval_input_reader[0].num_epochs = 1\n",
    "\n",
    "# Write to configuration file\n",
    "config_text = text_format.MessageToString(pipeline_config)\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:\n",
    "    f.write(config_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBWbfpEexiwZ"
   },
   "outputs": [],
   "source": [
    "#Use this for pretraind model from the pretrained model folder (downloaded from model zoo)\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "# Read current configuration file\n",
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:\n",
    "    proto_str = f.read()\n",
    "    text_format.Merge(proto_str, pipeline_config)\n",
    "\n",
    "# Update based on new labels\n",
    "pipeline_config.model.ssd.num_classes = len(labels)\n",
    "#pipeline_config.train_config.batch_size = 4\n",
    "#pipeline_config.train_config.batch_size = 3\n",
    "pipeline_config.train_config.batch_size = 2\n",
    "#pipeline_config.train_config.batch_size = 1\n",
    "\n",
    "#if the batch size is small the learning rate needs to be reduced for the model to converge\n",
    "#https://github.com/tensorflow/models/issues/9151\n",
    "#https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e\n",
    "pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.learning_rate_base = 0.03 #0.08\n",
    "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= files['LABELMAP']\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]\n",
    "pipeline_config.eval_input_reader[0].num_epochs = 1\n",
    "\n",
    "# Write to configuration file\n",
    "config_text = text_format.MessageToString(pipeline_config)\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:\n",
    "    f.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b4MBsUO0koz"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.1.2.30 (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68)\n",
    "ERROR: No matching distribution found for opencv-python-headless==4.1.2.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_tcn61DtpdV"
   },
   "outputs": [],
   "source": [
    "# fix opencv-python version issue \n",
    "#!pip uninstall opencv-python-headless \n",
    "#!pip install opencv-python-headless==4.1.2.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tt9OuLm10o2G",
    "outputId": "6d0d4be1-b689-4ee9-b8cf-f9660efae473",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 11:09:51.567771: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-10 11:09:52.734289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1347 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 960, pci bus id: 0000:01:00.0, compute capability: 5.2\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "I0210 11:09:52.924170 27428 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: 2000\n",
      "I0210 11:09:52.934169 27428 config_util.py:552] Maybe overwriting train_steps: 2000\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0210 11:09:52.934169 27428 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\object_detection\\model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W0210 11:09:52.971169 27428 deprecation.py:350] From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\object_detection\\model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['E:\\\\Users\\\\Vince\\\\Datasets\\\\NZRC\\\\Gordon\\\\CycloneGitaRawImagery\\\\ML4DR_20210910_01\\\\tiles\\\\train.record']\n",
      "I0210 11:09:52.987170 27428 dataset_builder.py:162] Reading unweighted datasets: ['E:\\\\Users\\\\Vince\\\\Datasets\\\\NZRC\\\\Gordon\\\\CycloneGitaRawImagery\\\\ML4DR_20210910_01\\\\tiles\\\\train.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['E:\\\\Users\\\\Vince\\\\Datasets\\\\NZRC\\\\Gordon\\\\CycloneGitaRawImagery\\\\ML4DR_20210910_01\\\\tiles\\\\train.record']\n",
      "I0210 11:09:52.987170 27428 dataset_builder.py:79] Reading record datasets for input file: ['E:\\\\Users\\\\Vince\\\\Datasets\\\\NZRC\\\\Gordon\\\\CycloneGitaRawImagery\\\\ML4DR_20210910_01\\\\tiles\\\\train.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I0210 11:09:52.987170 27428 dataset_builder.py:80] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0210 11:09:52.988170 27428 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W0210 11:09:53.000171 27428 deprecation.py:350] From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0210 11:09:53.031172 27428 deprecation.py:350] From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W0210 11:10:00.058172 27428 deprecation.py:350] From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W0210 11:10:03.238194 27428 deprecation.py:350] From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0210 11:10:04.990172 27428 deprecation.py:350] From C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Users\\Vince\\Documents\\GitHub\\object_detection_using_tensorflow\\Tensorflow\\models\\research\\object_detection\\model_main_tf2.py\", line 114, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 36, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\absl\\app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\absl\\app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"D:\\Users\\Vince\\Documents\\GitHub\\object_detection_using_tensorflow\\Tensorflow\\models\\research\\object_detection\\model_main_tf2.py\", line 105, in main\n",
      "    model_lib_v2.train_loop(\n",
      "  File \"C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 602, in train_loop\n",
      "    variables_helper.ensure_checkpoint_supported(\n",
      "  File \"C:\\Users\\Vince\\miniconda3\\envs\\tf2_gpu\\lib\\site-packages\\object_detection\\utils\\variables_helper.py\", line 227, in ensure_checkpoint_supported\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Checkpoint dir (D:\\Users\\Vince\\Documents\\GitHub\\object_detection_using_tensorflow\\Tensorflow\\workspace\\models\\my_ssd_mobnet_640) and model_dir (D:\\Users\\Vince\\Documents\\GitHub\\object_detection_using_tensorflow\\Tensorflow\\workspace\\models\\my_ssd_mobnet_640) cannot be same. Please set model_dir to a different path.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')\n",
    "command = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=2000\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'])\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:Couldn't match files for checkpoint Tensorflow\\workspace\\models\\my_ssd_mobnet_640\\ckpt-3\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#p = os.path.join(paths['CHECKPOINT_PATH'], 'checkpoint2')\n",
    "p = os.path.join(paths['CHECKPOINT_PATH'])\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(p)\n",
    "print(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training details\n",
    "\n",
    "#   https://www.tensorflow.org/guide/distributed_training\n",
    "#   https://www.tensorflow.org/guide/checkpoint\n",
    "\n",
    "#model_main_tf2.py\n",
    "#main()\n",
    "#   model_lib_v2.train_loop()\n",
    "\n",
    "#model_lib_v2.py\n",
    "#   training_loop()\n",
    "#      train_step_fn()\n",
    "#         eager_train_step()\n",
    "#            _compute_losses_and_predictions_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrXjxgnp4I5t"
   },
   "source": [
    "# (Optional) Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0huXcblU0uYm"
   },
   "outputs": [],
   "source": [
    "# stop evaluation if needed\n",
    "#command = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'])\n",
    "#!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXU31dxv5hwP"
   },
   "source": [
    "# Predicting object category in image\n",
    "\n",
    "### Load trained model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7a5XB5pQ4RzX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "#ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'checkpoint1', 'ckpt-2')).expect_partial()\n",
    "ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-3')).expect_partial()\n",
    "\n",
    "# @tf.function\n",
    "def detect_fn(image):\n",
    "    print(image.shape)\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    print(image.shape, shapes.shape)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xMF_w1854yf"
   },
   "source": [
    "### Detect objects in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "hCDa4sGs534v",
    "outputId": "42be4cac-9a22-4076-8980-56544357ea25"
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Randomly select an image to be detected\n",
    "test_path = os.path.join(paths['IMAGE_PATH'])\n",
    "tmp_img = random.choice([file for file in os.listdir(test_path) if file.endswith(\".JPG\")])\n",
    "IMAGE_PATH = os.path.join(test_path, tmp_img)\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(files['LABELMAP'])\n",
    "\n",
    "img = cv2.imread(IMAGE_PATH)\n",
    "image_np = np.array(img)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "detections = detect_fn(input_tensor)\n",
    "\n",
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy()\n",
    "              for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections\n",
    "\n",
    "# detection_classes should be ints.\n",
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "label_id_offset = 1\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np_with_detections,\n",
    "            detections['detection_boxes'],\n",
    "            detections['detection_classes']+label_id_offset,\n",
    "            detections['detection_scores'],\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=5,\n",
    "            min_score_thresh=.5, #(0.5)\n",
    "            agnostic_mode=False)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections['detection_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections['detection_boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, c = image_np.shape\n",
    "print(x,y,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detections['detection_boxes'][0:6]*[x,y,x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detections['detection_scores'][0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(detections))\n",
    "print(list(detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detections['detection_classes']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_detections)\n",
    "print(len(detections['detection_scores']))\n",
    "print(len(detections['detection_boxes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(paths['IMAGE_PATH'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNI02+tNFR8Pq6E5CJhl8VG",
   "collapsed_sections": [
    "KdyE_2lbPOKH",
    "zEv7XDo7nlpK"
   ],
   "include_colab_link": true,
   "name": "Training Customised Image Detection Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
